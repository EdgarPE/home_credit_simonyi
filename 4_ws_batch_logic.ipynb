{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch prediction notebook\n",
    "\n",
    "### Intro\n",
    "\n",
    "In this notebook we will see how we can utilize a trained model to create batch predictions for new data.\n",
    "The necessary steps are the following:\n",
    "\n",
    "1. Train and serialize a model based\n",
    "2. Package the trained model and scoring logic in a Docker container\n",
    "3. Deploy that container to a compute engine instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training and serializing a model\n",
    "\n",
    "As a preparation for the workshop today we need to update our environment with the correct python packages. \n",
    "\n",
    "**Restart the kernel afterwards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge scikit-learn==0.20.4 oauth2client pandas-gbq -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"norbert-liki-sandbox\"  # REPLACE THIS WITH YOUR PROJECT NAME!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "train = pd.read_csv('gs://home-credit-simonyi-workshop/input/application_train.subsample.csv')\n",
    "\n",
    "print('Train dataset shape (rows, columns): ', train.shape)\n",
    "\n",
    "target = 'TARGET'\n",
    "\n",
    "features = [\n",
    "    'DAYS_EMPLOYED',\n",
    "    'DAYS_BIRTH',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'CNT_FAM_MEMBERS',\n",
    "    'AMT_ANNUITY',\n",
    "    'EXT_SOURCE_1',\n",
    "    'EXT_SOURCE_2',\n",
    "    'EXT_SOURCE_3',\n",
    "    'NAME_TYPE_SUITE', # categorical\n",
    "    'NAME_INCOME_TYPE', # categorical\n",
    "]\n",
    "\n",
    "X = train.loc[:, features]\n",
    "y = train.loc[:, target]\n",
    "\n",
    "print(\"Train features DataFrame shape:\", X.shape)\n",
    "print(\"Train target Series shape:\", y.shape)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=train[target], test_size=0.5, random_state=seed)\n",
    "\n",
    "print('Train features shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Validate features shape: ', X_valid.shape)\n",
    "print('Validate target shape: ', y_valid.shape)\n",
    "\n",
    "\n",
    "num_feats = list(range(0, 9))\n",
    "num_cats = [9,10]\n",
    "num_transform = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "cat_transform = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transform, num_feats),\n",
    "    ('cat', cat_transform, num_cats)\n",
    "])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=3))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Check model performance on validation set\n",
    "\n",
    "metrics = [\n",
    "    ('Precision', precision_score, False),\n",
    "    ('Recall', recall_score, False),\n",
    "     ('ROC-AUC', roc_auc_score, True)\n",
    "]\n",
    "\n",
    "pred_valid = pipe.predict(X_valid)\n",
    "proba_valid = pipe.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print('-'*15, 'Model performance', '.'*15)\n",
    "\n",
    "for m in metrics:\n",
    "    score = m[1](y_valid, proba_valid) if m[2] else m[1](y_valid, pred_valid)\n",
    "    print('%s on CV: %.3f' % (m[0], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Serializing the model\n",
    "\n",
    "We need to seralize our model in some format. There are many ways how we can do it. With scikit-learn models we can use either pickle or joblib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('batch_prediction_src/trained_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to import it and see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('batch_prediction_src/trained_pipe.pkl', 'rb') as f:\n",
    "    old_pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = old_pipe.predict(X_valid)\n",
    "proba_valid = old_pipe.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print('-'*15, 'Model performance', '.'*15)\n",
    "\n",
    "for m in metrics:\n",
    "    score = m[1](y_valid, proba_valid) if m[2] else m[1](y_valid, pred_valid)\n",
    "    print('%s on CV: %.3f' % (m[0], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating our prediction pipeline \n",
    "\n",
    "### 2.1 Creating our scoring function\n",
    "\n",
    "\n",
    "In the following sections we will create the logic that will execute the prediction on new data.\n",
    "\n",
    "It will have 3 main steps:\n",
    "1. Load new data from Cloud Storage\n",
    "2. Score the predictions\n",
    "3. Upload the predictions to BigQuery.\n",
    "\n",
    "We will create it as a python script using ipython magic function *writefile*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_prediction_src/scoring.py\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import datetime\n",
    "# import some functions we need to shutdown the VM if we are running in Google Cloud\n",
    "from shutdown import kill_vm\n",
    "import atexit\n",
    "\n",
    "\n",
    "# load new data from Cloud Storage\n",
    "input_data = pd.read_csv('gs://home-credit-simonyi-workshop/input/application_train.subsample.csv')\n",
    "\n",
    "\n",
    "# load our saved pipeline pickle file.\n",
    "try:\n",
    "    with open('trained_pipe.pkl', 'rb') as f:\n",
    "        pipe = pickle.load(f)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    with open('batch_prediction_src/trained_pipe.pkl', 'rb') as f:\n",
    "            pipe = pickle.load(f)\n",
    "except:\n",
    "    print('Model not found.')\n",
    "    \n",
    "        \n",
    "\n",
    "# Define our feature columns\n",
    "feature_cols = [\n",
    "    'DAYS_EMPLOYED',\n",
    "    'DAYS_BIRTH',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'CNT_FAM_MEMBERS',\n",
    "    'AMT_ANNUITY',\n",
    "    'EXT_SOURCE_1',\n",
    "    'EXT_SOURCE_2',\n",
    "    'EXT_SOURCE_3',\n",
    "    'NAME_TYPE_SUITE', # categorical\n",
    "    'NAME_INCOME_TYPE', # categorical\n",
    "]\n",
    "    \n",
    "# Create the predictions and add them to the input dataframe.\n",
    "input_data = input_data.assign(prediction=pipe.predict_proba(input_data[feature_cols])[:,1],\n",
    "                               time=datetime.datetime.utcnow())\n",
    "\n",
    "# Create our final result dataframe\n",
    "out_data = input_data[['SK_ID_CURR', 'prediction','time']]\n",
    "\n",
    "# Upload it to BigQuery.\n",
    "bq_table = 'simonyi_ml.prediction_scores'\n",
    "pandas_gbq.to_gbq(dataframe=out_data,\n",
    "                  destination_table=bq_table,\n",
    "                  project_id='norbert-liki-sandbox',   ## CHANGE THIS TO YOUR PROJECT_ID\n",
    "                  if_exists='append')\n",
    "\n",
    "print('Success.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to run our scoring script and check its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python batch_prediction_src/scoring.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq\n",
    "\n",
    "def validate_bq_results():\n",
    "    bq_table = 'simonyi_ml.prediction_scores'\n",
    "    query = f\"select * from {bq_table}\"\n",
    "\n",
    "    check_df = pandas_gbq.read_gbq(query,project_id=PROJECT_ID)\n",
    "    print(check_df.head())\n",
    "    print('-'*15, 'Prediction records by date.')\n",
    "    print(check_df.groupby('time').size())\n",
    "    \n",
    "validate_bq_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append a simple line at the end in order to stop the VM when the scoring has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_prediction_src/scoring.py -a\n",
    "\n",
    "atexit.register(kill_vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Packaging our scoring logic in Docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Docker to package up our scoring function.\n",
    "Docker has several benefits and its use is widespread in the IT industry:\n",
    "- Manage applications, not machines\n",
    "- Code works the same everywhere:\n",
    "        + Across dev, test and production\n",
    "        + Across bare-metal, VMs and cloud\n",
    "\n",
    "\n",
    "- Packaged apps speed development:\n",
    "        + Agile creation and deployment\n",
    "        + Continuous integration and delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](batch_prediction_src/container.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_prediction_src/Dockerfile\n",
    "FROM continuumio/miniconda3 as builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define our base container image that we are going to use. Docker containers consist of layers that we can stack and build upon. With this we can resuse already existing containers within an organization. For example we can create a base container for our organization which holds all the security options preconfigured so we do not have to manage them. We just need care about our application.\n",
    "\n",
    "\n",
    "In our case we start from a miniconda image which has the essentials installed for our python data project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add a file to our container and create our python environment\n",
    "- **conda.yaml** contains the conda environment description how to create our python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_prediction_src/Dockerfile -a\n",
    "\n",
    "ADD conda_env.yaml /\n",
    "RUN conda env create -f conda_env.yaml && \\\n",
    "    conda clean -a -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce image size we are building the container in two steps. As a next step we add more files to our image:\n",
    "- **scoring.py** contains the main scoring logic in python\n",
    "- **shutdown.py** contains the logic how to shutdown a Compute Engine Instance\n",
    "- **trained_pipe.pkl** our trained scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_prediction_src/Dockerfile -a\n",
    "\n",
    "FROM builder\n",
    "ADD scoring.py shutdown.py trained_pipe.pkl /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define and ENTRYPOINT for our image. It will make it executable and it will run once the container is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_prediction_src/Dockerfile -a\n",
    "\n",
    "ENTRYPOINT [\"conda\", \"run\", \"-n\", \"simonyi_workshop\", \"python\", \"scoring.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building our container with Cloud Build\n",
    "\n",
    "Once our container definition is ready, we need to build it. We can use Cloud Build service for that. It will store the image in Google Container Registry for later use. Using Cloud Build is easy but we need to modify the configuration a bit:\n",
    "\n",
    "1. Create *cloudbuild.ylm* to define our build steps and output containers\n",
    "2. Add *.gcloudignore* to explicitly include our trained_pipe.pkl in the container. Otherwise it would exclude it.\n",
    "3. Submit the build command using *gcloud cli*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile batch_prediction_src/cloudbuild.yml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/home_credit_scoring', 'batch_prediction_src/']\n",
    "options:\n",
    "  machineType: 'N1_HIGHCPU_8'\n",
    "tags: ['latest']\n",
    "images: ['gcr.io/$PROJECT_ID/home_credit_scoring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .gcloudignore\n",
    "\n",
    "!trainer_pipe.pkl\n",
    "*.ipynb\n",
    "*ipnb_checkpoints\n",
    "__pycache__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the build with the commands below. \n",
    "In the meantime open Cloud build from the console and observe the status there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --config batch_prediction_src/cloudbuild.yml  --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying the image to a Compute Engine instance\n",
    "\n",
    "Using our freshly built container we can deploy it as a background running service simply with a simple gcloud command.\n",
    "Note that we are using the preemptbile flag. With this we can save money. These instances cost almost 80% less than normal ones. On the other hand they could be shutdown anytime and stay up maximum 24 hours.\n",
    "\n",
    "**You can use this logic to create any long running computation in the background.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud compute instances create-with-container credit-scoring \\\n",
    "--container-image=\"gcr.io/norbert-liki-sandbox/home_credit_scoring:latest\" \\  # CHANGE THE PROJECT_ID TO YOUR PROJECT\n",
    "--zone=europe-west4-a \\\n",
    "--scopes=https://www.googleapis.com/auth/cloud-platform \\\n",
    "--maintenance-policy=TERMINATE \\\n",
    "--preemptible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_bq_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
