{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google AI Platform for model Deployment\n",
    "\n",
    "AI Platfrom Prediction service allows you to host your models in Google cloud and make them available as an API for online prediction or use them for native batch prediction in case of Tensorflow models.\n",
    "In this tutorial we are going to see how it is done when using our scikit-learn model.\n",
    "\n",
    "Using AI Platform for model deployment saves you the time to:\n",
    "- package your model to a web framework like Flask\n",
    "- you do not need to manage the infrastructure\n",
    "- your API will autoscale based on traffic\n",
    "\n",
    "In this notebook you will see:\n",
    "- how to format your training application with Pipelines in order to make them deployable in AI Platform\n",
    "- how to deploy your model\n",
    "- how to create a model version\n",
    "- how to get online predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for tuning\n",
    "\n",
    "Using the same commands from the training notebook we prepare a bucket and upload our trained model there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'norbert-liki-sandbox'\n",
    "BUCKET_NAME = 'simonyi-nl-online-serving-training'  # MODIFY THIS. IT NEEDS TO BE GLOBALLY UNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l EU -p $PROJECT_NAME gs://$BUCKET_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the upload we rename our model. It must have a name of *model.pkl* or *model.joblib* if using the standard deployment process. In custom prediction routine you can define your own name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp batch_prediction_src/trained_pipe.pkl gs://$BUCKET_NAME/model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create trainer application package\n",
    "\n",
    "In order to make our scikit-learn Pipeline deployable and serveble in AI Platform we need to modify our original training application. The reason behind it is that a deployed model expects data for prediction in a form of JSON requests and the features as lists. \n",
    "\n",
    "What does it mean for us? \n",
    "- If you don't use any transformation on your data just build a model then there are no complications for you\n",
    "- if you would like to transform your data at prediction time using Pipelines, then you need to create your initial model and training Pipeline to work on lists. (i.e. lists do not have column names) We will see how it is done.\n",
    "- if you want to have custom Transformers in your Pipeline then you you should use custom prediction routines.\n",
    "\n",
    "The caveats above does not apply for Tensorflow models because you can define different training and serving input functions in them.\n",
    "\n",
    "We created our model with these restrictions in our mind. **In our transformation pipeline we have been using indexes not column names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = list(range(0, 9))\n",
    "num_cats = [9,10]\n",
    "num_transform = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "# Columns can be accessed with names also.\n",
    "cat_feats = ['NAME_TYPE_SUITE', 'NAME_INCOME_TYPE'] \n",
    "cat_transform = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transform, num_feats),\n",
    "    ('cat', cat_transform, num_cats)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve the model\n",
    "Once the model is successfully created and trained, you can serve it. A model can have different versions. In order to serve the model, create a model and version in AI Platform. This terminology can be confusing because an AI Platform model resource is not actually a machine-learning model on its own. In Cloud ML Engine a model is a container for the versions of the machine learning model.\n",
    "\n",
    "Define the model and version names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "MODEL_NAME = \"serving_training\"\n",
    "VERSION_NAME = \"serving_training{}\".format(int(time.time())); VERSION_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model in AI Platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create $MODEL_NAME --regions europe-west1 --project norbert-liki-sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a version that points to your model file in Cloud Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform versions create $VERSION_NAME \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --framework=scikit-learn \\\n",
    "  --origin=gs://$BUCKET_NAME/ \\\n",
    "  --python-version=3.7 \\\n",
    "  --runtime-version=1.15 \\\n",
    "  --project=norbert-liki-sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making online predictions\n",
    "\n",
    "### Format data for prediction\n",
    "\n",
    "Before you send an online prediction request, you must format your test data to prepare it for use by the AI Platform Prediction service. Make sure that the format of your input instances matches what your model expects.\n",
    "\n",
    "We will use a few samples from our training data for our presentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "train = pd.read_csv('gs://home-credit-simonyi-workshop/input/application_train.subsample.csv', nrows=100)\n",
    "\n",
    "target = 'TARGET'\n",
    "\n",
    "features = [\n",
    "    'DAYS_EMPLOYED',\n",
    "    'DAYS_BIRTH',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'CNT_FAM_MEMBERS',\n",
    "    'AMT_ANNUITY',\n",
    "    'EXT_SOURCE_1',\n",
    "    'EXT_SOURCE_2',\n",
    "    'EXT_SOURCE_3',\n",
    "    'NAME_TYPE_SUITE', # categorical\n",
    "    'NAME_INCOME_TYPE', # categorical\n",
    "]\n",
    "\n",
    "X = train.loc[:, features]\n",
    "y = train.loc[:, target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the online prediction request\n",
    "\n",
    "Using Google's Python API we can easily call our model endpoint to recieve predictions. We just need to parse our data into an appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "\n",
    "instances = X.values[:10].tolist()  # we are transforming the data here in the expected format\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}/versions/{}'.format(PROJECT_NAME, MODEL_NAME,\n",
    "                                                  VERSION_NAME)\n",
    "\n",
    "response = service.projects().predict(\n",
    "    name=name,\n",
    "    body={'instances': instances}\n",
    ").execute()\n",
    "\n",
    "\n",
    "response['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The prediction output in our case returns class assignments/labels and not probabilities since this is the default behaviour in the predict method in scikit-learn.\n",
    "If we want to have probabilities instead we could overwrite the predict class of our trained pipeline with our expected answer. E.g.:\n",
    "\n",
    "```pipe.predict = pipe.predict_proba[:,1]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil rm -r gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform versions delete $VERSION_NAME \\\n",
    "--model $MODEL_NAME \\\n",
    "--quiet \\\n",
    "--project $PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform models delete $MODEL_NAME \\\n",
    "--quiet \\\n",
    "--project $PROJECT_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
