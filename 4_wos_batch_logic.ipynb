{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch prediction notebook\n",
    "\n",
    "### Intro\n",
    "\n",
    "In this notebook we will see how we can utilize a trained model to create batch predictions for new data.\n",
    "The necessary steps are the following:\n",
    "\n",
    "1. Train and serialize a model based\n",
    "2. Package the trained model and scoring logic in a Docker container\n",
    "3. Deploy that container to a compute engine instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training and serializing a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape (rows, columns):  (2000, 123)\n",
      "Train features DataFrame shape: (2000, 11)\n",
      "Train target Series shape: (2000,)\n",
      "Train features shape:  (1000, 11)\n",
      "Train target shape:  (1000,)\n",
      "Validate features shape:  (1000, 11)\n",
      "Validate target shape:  (1000,)\n",
      "--------------- Model performance ...............\n",
      "Precision on CV: 0.000\n",
      "Recall on CV: 0.000\n",
      "ROC-AUC on CV: 0.676\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "train = pd.read_csv('gs://home-credit-simonyi-workshop/input/application_train.subsample.csv')\n",
    "\n",
    "print('Train dataset shape (rows, columns): ', train.shape)\n",
    "\n",
    "target = 'TARGET'\n",
    "\n",
    "features = [\n",
    "    'DAYS_EMPLOYED',\n",
    "    'DAYS_BIRTH',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'CNT_FAM_MEMBERS',\n",
    "    'AMT_ANNUITY',\n",
    "    'EXT_SOURCE_1',\n",
    "    'EXT_SOURCE_2',\n",
    "    'EXT_SOURCE_3',\n",
    "    'NAME_TYPE_SUITE', # categorical\n",
    "    'NAME_INCOME_TYPE', # categorical\n",
    "]\n",
    "\n",
    "X = train.loc[:, features]\n",
    "y = train.loc[:, target]\n",
    "\n",
    "print(\"Train features DataFrame shape:\", X.shape)\n",
    "print(\"Train target Series shape:\", y.shape)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=train[target], test_size=0.5, random_state=seed)\n",
    "\n",
    "print('Train features shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Validate features shape: ', X_valid.shape)\n",
    "print('Validate target shape: ', y_valid.shape)\n",
    "\n",
    "\n",
    "num_feats = list(range(0, 9))\n",
    "num_cats = [9,10]\n",
    "num_transform = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "# Columns can be accessed with names also.\n",
    "cat_feats = ['NAME_TYPE_SUITE', 'NAME_INCOME_TYPE'] \n",
    "cat_transform = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transform, num_feats),\n",
    "    ('cat', cat_transform, num_cats)\n",
    "])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=12))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Check model performance on validation set\n",
    "\n",
    "metrics = [\n",
    "    ('Precision', precision_score, False),\n",
    "    ('Recall', recall_score, False),\n",
    "#     ('MCC', matthews_corrcoef, False),\n",
    "#     ('F1', f1_score, False),\n",
    "     ('ROC-AUC', roc_auc_score, True)\n",
    "]\n",
    "\n",
    "pred_valid = pipe.predict(X_valid)\n",
    "proba_valid = pipe.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print('-'*15, 'Model performance', '.'*15)\n",
    "\n",
    "for m in metrics:\n",
    "    score = m[1](y_valid, proba_valid) if m[2] else m[1](y_valid, pred_valid)\n",
    "    print('%s on CV: %.3f' % (m[0], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Serializing the model\n",
    "\n",
    "We need to seralize our model in some format. There are two ways how we can do it. Either with pickle or with joblib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('trained_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to import it and see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_pipe.pkl', 'rb') as f:\n",
    "    old_pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Model performance ...............\n",
      "Precision on CV: 0.000\n",
      "Recall on CV: 0.000\n",
      "ROC-AUC on CV: 0.676\n"
     ]
    }
   ],
   "source": [
    "pred_valid = old_pipe.predict(X_valid)\n",
    "proba_valid = old_pipe.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print('-'*15, 'Model performance', '.'*15)\n",
    "\n",
    "for m in metrics:\n",
    "    score = m[1](y_valid, proba_valid) if m[2] else m[1](y_valid, pred_valid)\n",
    "    print('%s on CV: %.3f' % (m[0], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating our prediction pipeline \n",
    "\n",
    "### 2.1 Creating our scoring function\n",
    "\n",
    "\n",
    "In the following sections we will create the logic that will execute the prediction on new data.\n",
    "\n",
    "It will have 3 main steps:\n",
    "1. Load new data from Cloud Storage\n",
    "2. Score the predictions\n",
    "3. Upload the predictions to BigQuery.\n",
    "\n",
    "We will create it as a python script using ipython magic function *writefile*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scoring.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scoring.py\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import datetime\n",
    "# import some functions we need to shutdown the VM if we are running in Google Cloud\n",
    "from shutdown import kill_vm\n",
    "import atexit\n",
    "\n",
    "\n",
    "# load new data from Cloud Storage\n",
    "input_data = pd.read_csv('gs://home-credit-simonyi-workshop/input/application_train.subsample.csv')\n",
    "\n",
    "\n",
    "# load our saved pipeline pickle file.\n",
    "with open('trained_pipe.pkl', 'rb') as f:\n",
    "    pipe = pickle.load(f)\n",
    "\n",
    "# Define our feature columns\n",
    "feature_cols = [\n",
    "    'DAYS_EMPLOYED',\n",
    "    'DAYS_BIRTH',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'CNT_FAM_MEMBERS',\n",
    "    'AMT_ANNUITY',\n",
    "    'EXT_SOURCE_1',\n",
    "    'EXT_SOURCE_2',\n",
    "    'EXT_SOURCE_3',\n",
    "    'NAME_TYPE_SUITE', # categorical\n",
    "    'NAME_INCOME_TYPE', # categorical\n",
    "]\n",
    "    \n",
    "# Create the predictions and add them to the input dataframe.\n",
    "input_data = input_data.assign(prediction=pipe.predict_proba(input_data[feature_cols])[:,1],\n",
    "                               time=datetime.datetime.utcnow())\n",
    "\n",
    "# Create our final result dataframe\n",
    "out_data = input_data[['SK_ID_CURR', 'prediction','time']]\n",
    "\n",
    "# Upload it to BigQuery.\n",
    "bq_table = 'simonyi_ml.prediction_scores'\n",
    "pandas_gbq.to_gbq(dataframe=out_data,\n",
    "                  destination_table=bq_table,\n",
    "                  project_id='norbert-liki-sandbox',\n",
    "                  if_exists='append')\n",
    "\n",
    "print('Success.')\n",
    "\n",
    "atexit.register(kill_vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to run our scoring script and check its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/no_liki/anaconda3/envs/diepresse/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/home/no_liki/anaconda3/envs/diepresse/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "1it [00:09,  9.42s/it]\n",
      "Success.\n",
      "Not running inside a VM\n"
     ]
    }
   ],
   "source": [
    "!python scoring.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 12000/12000 [00:01<00:00, 9614.92rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SK_ID_CURR  prediction                             time\n",
      "0      268490    0.000000 2020-03-12 13:45:28.364581+00:00\n",
      "1      401057    0.033122 2020-03-12 13:45:28.364581+00:00\n",
      "2      166801    0.128110 2020-03-12 13:45:28.364581+00:00\n",
      "3      130052    0.018069 2020-03-12 13:45:28.364581+00:00\n",
      "4      224534    0.444930 2020-03-12 13:45:28.364581+00:00\n",
      "--------------- Prediction records by date.\n",
      "time\n",
      "2020-03-12 13:39:44.305047+00:00    2000\n",
      "2020-03-12 13:41:29.782480+00:00    2000\n",
      "2020-03-12 13:45:28.364581+00:00    2000\n",
      "2020-03-12 13:48:38.421967+00:00    2000\n",
      "2020-03-13 16:10:00.333668+00:00    2000\n",
      "2020-03-13 16:13:00.288943+00:00    2000\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas_gbq\n",
    "\n",
    "def validate_bq_results():\n",
    "    bq_table = 'simonyi_ml.prediction_scores'\n",
    "    query = f\"select * from {bq_table}\"\n",
    "\n",
    "    check_df = pandas_gbq.read_gbq(query,project_id='norbert-liki-sandbox')\n",
    "    print(check_df.head())\n",
    "    print('-'*15, 'Prediction records by date.')\n",
    "    print(check_df.groupby('time').size())\n",
    "    \n",
    "validate_bq_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Packaging our scoring logic in Docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Docker to package up our scoring function.\n",
    "Docker has several benefits and its use is widespread in the IT industry:\n",
    "- Manage applications, not machines\n",
    "- Code works the same everywhere:\n",
    "        + Across dev, test and production\n",
    "        + Across bare-metal, VMs and cloud\n",
    "\n",
    "\n",
    "- Packaged apps speed development:\n",
    "        + Agile creation and deployment\n",
    "        + Continuous integration and delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](container.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM continuumio/miniconda3 as builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define our base container image that we are going to use. Docker containers consist of layers that we can stack and build upon. With this we can resuse already existing containers within an organization. For example we can create a base container for our organization which holds all the security options preconfigured so we do not have to manage them. We just need care about our application.\n",
    "\n",
    "\n",
    "In our case we start from a miniconda image which has the essantials installed for our python data project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add a few files to our container:\n",
    "- **conda.yaml** contains the conda environment description how to create our python environment\n",
    "- **scoring.py** contains the main scoring logic in python\n",
    "- **shutdown.py** contains the logic how to shutdown a Compute Engine Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile -a\n",
    "\n",
    "ADD conda_env.yaml /\n",
    "RUN conda env create -f conda_env.yaml && /\n",
    "    conda clean -a -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we install the necessary python packages with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile -a\n",
    "\n",
    "FROM builder\n",
    "ADD scoring.py shutdown.py trained_pipe.pkl /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define and ENTRYPOINT for our image. It will make it executable and it will run once the container is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile -a\n",
    "\n",
    "ENTRYPOINT [\"conda\", \"run\", \"-n\", \"simonyi_workshop\", \"python\", \"scoring.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building our container with Cloud Build\n",
    "\n",
    "Once our container definition is ready, we need to build it. We can use Cloud Build service for that. It will store the image in Google Container Registry for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"norbert-liki-sandbox\"  # REPLACE THIS WITH YOUR PROJECT NAME!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 128 file(s) totalling 6.6 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://norbert-liki-sandbox_cloudbuild/source/1584123969.16-ad85cd62f5ba46f395e71cb256fe3bfb.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/norbert-liki-sandbox/builds/afbb3a86-cfdf-43ae-89da-723db5aac3a8].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/afbb3a86-cfdf-43ae-89da-723db5aac3a8?project=31878841857].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"afbb3a86-cfdf-43ae-89da-723db5aac3a8\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://norbert-liki-sandbox_cloudbuild/source/1584123969.16-ad85cd62f5ba46f395e71cb256fe3bfb.tgz#1584124018464635\n",
      "Copying gs://norbert-liki-sandbox_cloudbuild/source/1584123969.16-ad85cd62f5ba46f395e71cb256fe3bfb.tgz#1584124018464635...\n",
      "/ [1 files][  5.7 MiB/  5.7 MiB]                                                \n",
      "Operation completed over 1 objects/5.7 MiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  7.111MB\n",
      "Step 1/6 : FROM continuumio/miniconda3 as builder\n",
      "latest: Pulling from continuumio/miniconda3\n",
      "68ced04f60ab: Pulling fs layer\n",
      "9c388eb6d33c: Pulling fs layer\n",
      "96cf53b3a9dd: Pulling fs layer\n",
      "68ced04f60ab: Verifying Checksum\n",
      "68ced04f60ab: Download complete\n",
      "96cf53b3a9dd: Verifying Checksum\n",
      "96cf53b3a9dd: Download complete\n",
      "9c388eb6d33c: Verifying Checksum\n",
      "9c388eb6d33c: Download complete\n",
      "68ced04f60ab: Pull complete\n",
      "9c388eb6d33c: Pull complete\n",
      "96cf53b3a9dd: Pull complete\n",
      "Digest: sha256:456e3196bf3ffb13fee7c9216db4b18b5e6f4d37090b31df3e0309926e98cfe2\n",
      "Status: Downloaded newer image for continuumio/miniconda3:latest\n",
      " ---> b4adc22212f1\n",
      "Step 2/6 : ADD conda_env.yaml /\n",
      " ---> dc96247e482b\n",
      "Step 3/6 : RUN conda env create -f conda_env.yaml\n",
      " ---> Running in 2b8d4641ffd4\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "attrs-19.3.0         | 35 KB     | ########## | 100% \n",
      "pytest-5.3.5         | 366 KB    | ########## | 100% \n",
      "gettext-0.19.8.1     | 3.6 MB    | ########## | 100% \n",
      "py-1.8.1             | 66 KB     | ########## | 100% \n",
      "pexpect-4.8.0        | 79 KB     | ########## | 100% \n",
      "terminado-0.8.3      | 23 KB     | ########## | 100% \n",
      "qt-5.12.5            | 99.2 MB   | ########## | 100% \n",
      "krb5-1.16.4          | 1.4 MB    | ########## | 100% \n",
      "grpcio-1.23.0        | 1.1 MB    | ########## | 100% \n",
      "bzip2-1.0.8          | 396 KB    | ########## | 100% \n",
      "_openmp_mutex-4.5    | 5 KB      | ########## | 100% \n",
      "freetype-2.10.0      | 884 KB    | ########## | 100% \n",
      "seaborn-0.10.0       | 158 KB    | ########## | 100% \n",
      "google-cloud-bigquer | 10 KB     | ########## | 100% \n",
      "libllvm9-9.0.1       | 25.1 MB   | ########## | 100% \n",
      "pip-20.0.2           | 1.0 MB    | ########## | 100% \n",
      "jsonschema-3.2.0     | 89 KB     | ########## | 100% \n",
      "numpy-1.18.1         | 5.2 MB    | ########## | 100% \n",
      "urllib3-1.25.7       | 159 KB    | ########## | 100% \n",
      "oauthlib-3.0.1       | 82 KB     | ########## | 100% \n",
      "icu-64.2             | 12.6 MB   | ########## | 100% \n",
      "matplotlib-base-3.2. | 7.0 MB    | ########## | 100% \n",
      "google-cloud-bigquer | 8 KB      | ########## | 100% \n",
      "libcurl-7.68.0       | 564 KB    | ########## | 100% \n",
      "libcblas-3.8.0       | 10 KB     | ########## | 100% \n",
      "click-7.1.1          | 64 KB     | ########## | 100% \n",
      "libiconv-1.15        | 2.0 MB    | ########## | 100% \n",
      "pyopenssl-19.1.0     | 47 KB     | ########## | 100% \n",
      "arrow-cpp-0.16.0     | 19.8 MB   | ########## | 100% \n",
      "cycler-0.10.0        | 9 KB      | ########## | 100% \n",
      "widgetsnbextension-3 | 1.8 MB    | ########## | 100% \n",
      "pyasn1-modules-0.2.7 | 60 KB     | ########## | 100% \n",
      "google-cloud-bigquer | 54 KB     | ########## | 100% \n",
      "libpng-1.6.37        | 343 KB    | ########## | 100% \n",
      "parso-0.6.2          | 66 KB     | ########## | 100% \n",
      "scikit-learn-0.22.2. | 7.1 MB    | ########## | 100% \n",
      "pytz-2019.3          | 237 KB    | ########## | 100% \n",
      "libgfortran-ng-7.3.0 | 1.7 MB    | ########## | 100% \n",
      "libgcc-ng-9.2.0      | 8.2 MB    | ########## | 100% \n",
      "importlib_metadata-1 | 3 KB      | ########## | 100% \n",
      "pysocks-1.7.1        | 27 KB     | ########## | 100% \n",
      "google-cloud-bigquer | 229 KB    | ########## | 100% \n",
      "jinja2-2.11.1        | 94 KB     | ########## | 100% \n",
      "fontconfig-2.13.1    | 340 KB    | ########## | 100% \n",
      "libxml2-2.9.10       | 1.3 MB    | ########## | 100% \n",
      "mccabe-0.6.1         | 8 KB      | ########## | 100% \n",
      "xorg-libxau-1.0.9    | 13 KB     | ########## | 100% \n",
      "libllvm8-8.0.1       | 23.2 MB   | ########## | 100% \n",
      "libxcb-1.13          | 396 KB    | ########## | 100% \n",
      "astropy-4.0          | 7.4 MB    | ########## | 100% \n",
      "pyarrow-0.16.0       | 4.1 MB    | ########## | 100% \n",
      "certifi-2019.11.28   | 149 KB    | ########## | 100% \n",
      "decorator-4.4.2      | 11 KB     | ########## | 100% \n",
      "llvm-openmp-9.0.1    | 782 KB    | ########## | 100% \n",
      "google-resumable-med | 31 KB     | ########## | 100% \n",
      "phik-0.9.10          | 583 KB    | ########## | 100% \n",
      "jedi-0.16.0          | 778 KB    | ########## | 100% \n",
      "pycparser-2.20       | 89 KB     | ########## | 100% \n",
      "rsa-4.0              | 27 KB     | ########## | 100% \n",
      "kiwisolver-1.1.0     | 86 KB     | ########## | 100% \n",
      "patsy-0.5.1          | 187 KB    | ########## | 100% \n",
      "python-3.7.6         | 52.9 MB   | ########## | 100% \n",
      "pluggy-0.13.0        | 28 KB     | ########## | 100% \n",
      "libssh2-1.8.2        | 257 KB    | ########## | 100% \n",
      "xz-5.2.4             | 366 KB    | ########## | 100% \n",
      "tk-8.6.10            | 3.2 MB    | ########## | 100% \n",
      "backcall-0.1.0       | 13 KB     | ########## | 100% \n",
      "importlib-metadata-1 | 42 KB     | ########## | 100% \n",
      "astroid-2.3.3        | 279 KB    | ########## | 100% \n",
      "pandas-1.0.2         | 11.1 MB   | ########## | 100% \n",
      "webencodings-0.5.1   | 12 KB     | ########## | 100% \n",
      "entrypoints-0.3      | 12 KB     | ########## | 100% \n",
      "tornado-6.0.4        | 639 KB    | ########## | 100% \n",
      "gst-plugins-base-1.1 | 6.8 MB    | ########## | 100% \n",
      "curl-7.68.0          | 137 KB    | ########## | 100% \n",
      "nss-3.47             | 1.9 MB    | ########## | 100% \n",
      "libxkbcommon-0.10.0  | 475 KB    | ########## | 100% \n",
      "yaml-0.2.2           | 82 KB     | ########## | 100% \n",
      "readline-8.0         | 441 KB    | ########## | 100% \n",
      "missingno-0.4.2      | 12 KB     | ########## | 100% \n",
      "snappy-1.1.8         | 39 KB     | ########## | 100% \n",
      "libblas-3.8.0        | 10 KB     | ########## | 100% \n",
      "chardet-3.0.4        | 167 KB    | ########## | 100% \n",
      "ptyprocess-0.6.0     | 15 KB     | ########## | 100% \n",
      "htmlmin-0.1.12       | 21 KB     | ########## | 100% \n",
      "python-dateutil-2.8. | 220 KB    | ########## | 100% \n",
      "nbformat-5.0.4       | 98 KB     | ########## | 100% \n",
      "requests-2.23.0      | 85 KB     | ########## | 100% \n",
      "libuuid-2.32.1       | 26 KB     | ########## | 100% \n",
      "zeromq-4.3.2         | 668 KB    | ########## | 100% \n",
      "prometheus_client-0. | 38 KB     | ########## | 100% \n",
      "confuse-1.0.0        | 19 KB     | ########## | 100% \n",
      "google-auth-oauthlib | 18 KB     | ########## | 100% \n",
      "ncurses-6.1          | 1.3 MB    | ########## | 100% \n",
      "pickleshare-0.7.5    | 12 KB     | ########## | 100% \n",
      "lz4-c-1.8.3          | 187 KB    | ########## | 100% \n",
      "libprotobuf-3.11.4   | 4.8 MB    | ########## | 100% \n",
      "statsmodels-0.11.1   | 10.1 MB   | ########## | 100% \n",
      "pytest-pylint-0.14.1 | 9 KB      | ########## | 100% \n",
      "pyzmq-19.0.0         | 489 KB    | ########## | 100% \n",
      "wheel-0.34.2         | 24 KB     | ########## | 100% \n",
      "jupyter_console-6.1. | 21 KB     | ########## | 100% \n",
      "gstreamer-1.14.5     | 4.5 MB    | ########## | 100% \n",
      "isort-4.3.21         | 64 KB     | ########## | 100% \n",
      "jupyter_client-6.0.0 | 71 KB     | ########## | 100% \n",
      "libedit-3.1.20170329 | 172 KB    | ########## | 100% \n",
      "testpath-0.4.4       | 85 KB     | ########## | 100% \n",
      "ipywidgets-7.5.1     | 101 KB    | ########## | 100% \n",
      "libsodium-1.0.17     | 330 KB    | ########## | 100% \n",
      "cachetools-3.1.1     | 11 KB     | ########## | 100% \n",
      "googleapis-common-pr | 64 KB     | ########## | 100% \n",
      "defusedxml-0.6.0     | 22 KB     | ########## | 100% \n",
      "notebook-6.0.3       | 6.2 MB    | ########## | 100% \n",
      "llvmlite-0.31.0      | 325 KB    | ########## | 100% \n",
      "tqdm-4.43.0          | 47 KB     | ########## | 100% \n",
      "fsspec-0.6.2         | 46 KB     | ########## | 100% \n",
      "qtpy-1.9.0           | 34 KB     | ########## | 100% \n",
      "google-api-core-1.16 | 83 KB     | ########## | 100% \n",
      "pandocfilters-1.4.2  | 9 KB      | ########## | 100% \n",
      "pandas-gbq-0.13.1    | 22 KB     | ########## | 100% \n",
      "numba-0.48.0         | 3.4 MB    | ########## | 100% \n",
      "google-cloud-core-1. | 24 KB     | ########## | 100% \n",
      "_libgcc_mutex-0.1    | 3 KB      | ########## | 100% \n",
      "traitlets-4.3.3      | 133 KB    | ########## | 100% \n",
      "requests-oauthlib-1. | 19 KB     | ########## | 100% \n",
      "bleach-3.1.1         | 111 KB    | ########## | 100% \n",
      "mistune-0.8.4        | 53 KB     | ########## | 100% \n",
      "gcsfs-0.6.0          | 19 KB     | ########## | 100% \n",
      "protobuf-3.11.4      | 699 KB    | ########## | 100% \n",
      "ca-certificates-2019 | 145 KB    | ########## | 100% \n",
      "libopenblas-0.3.8    | 7.8 MB    | ########## | 100% \n",
      "prompt_toolkit-3.0.4 | 4 KB      | ########## | 100% \n",
      "pandoc-2.9.2         | 16.8 MB   | ########## | 100% \n",
      "parquet-cpp-1.5.1    | 3 KB      | ########## | 100% \n",
      "xorg-libxdmcp-1.1.3  | 18 KB     | ########## | 100% \n",
      "ipython_genutils-0.2 | 21 KB     | ########## | 100% \n",
      "zstd-1.4.4           | 989 KB    | ########## | 100% \n",
      "nbconvert-5.6.1      | 487 KB    | ########## | 100% \n",
      "nspr-4.25            | 1.6 MB    | ########## | 100% \n",
      "more-itertools-8.2.0 | 35 KB     | ########## | 100% \n",
      "gflags-2.2.2         | 175 KB    | ########## | 100% \n",
      "pthread-stubs-0.4    | 5 KB      | ########## | 100% \n",
      "aws-sdk-cpp-1.7.164  | 1.8 MB    | ########## | 100% \n",
      "pygments-2.6.1       | 683 KB    | ########## | 100% \n",
      "scipy-1.4.1          | 18.8 MB   | ########## | 100% \n",
      "pandas-profiling-2.4 | 134 KB    | ########## | 100% \n",
      "google-auth-1.11.2   | 47 KB     | ########## | 100% \n",
      "jupyter-1.0.0        | 4 KB      | ########## | 100% \n",
      "qtconsole-4.7.1      | 87 KB     | ########## | 100% \n",
      "zlib-1.2.11          | 105 KB    | ########## | 100% \n",
      "cryptography-2.8     | 625 KB    | ########## | 100% \n",
      "packaging-20.1       | 31 KB     | ########## | 100% \n",
      "libclang-9.0.1       | 22.3 MB   | ########## | 100% \n",
      "python_abi-3.7       | 4 KB      | ########## | 100% \n",
      "thrift-cpp-0.13.0    | 18.6 MB   | ########## | 100% \n",
      "wrapt-1.12.1         | 46 KB     | ########## | 100% \n",
      "brotli-1.0.7         | 1.0 MB    | ########## | 100% \n",
      "pyasn1-0.4.8         | 53 KB     | ########## | 100% \n",
      "ld_impl_linux-64-2.3 | 589 KB    | ########## | 100% \n",
      "send2trash-1.5.0     | 12 KB     | ########## | 100% \n",
      "markupsafe-1.1.1     | 26 KB     | ########## | 100% \n",
      "google-api-core-grpc | 5 KB      | ########## | 100% \n",
      "wcwidth-0.1.8        | 19 KB     | ########## | 100% \n",
      "liblapack-3.8.0      | 10 KB     | ########## | 100% \n",
      "pyyaml-5.3           | 184 KB    | ########## | 100% \n",
      "prompt-toolkit-3.0.4 | 233 KB    | ########## | 100% \n",
      "six-1.14.0           | 13 KB     | ########## | 100% \n",
      "pytest-runner-5.2    | 9 KB      | ########## | 100% \n",
      "blinker-1.4          | 13 KB     | ########## | 100% \n",
      "openssl-1.1.1d       | 2.1 MB    | ########## | 100% \n",
      "lazy-object-proxy-1. | 27 KB     | ########## | 100% \n",
      "fastavro-0.22.13     | 415 KB    | ########## | 100% \n",
      "ipython-7.13.0       | 1.1 MB    | ########## | 100% \n",
      "zipp-3.1.0           | 10 KB     | ########## | 100% \n",
      "sqlite-3.30.1        | 2.0 MB    | ########## | 100% \n",
      "re2-2020.03.03       | 435 KB    | ########## | 100% \n",
      "pyqt-5.12.3          | 6.3 MB    | ########## | 100% \n",
      "joblib-0.14.1        | 198 KB    | ########## | 100% \n",
      "pcre-8.44            | 261 KB    | ########## | 100% \n",
      "jpeg-9c              | 251 KB    | ########## | 100% \n",
      "matplotlib-3.2.0     | 6 KB      | ########## | 100% \n",
      "pyparsing-2.4.6      | 59 KB     | ########## | 100% \n",
      "abseil-cpp-20190808  | 818 KB    | ########## | 100% \n",
      "dbus-1.13.6          | 602 KB    | ########## | 100% \n",
      "pyjwt-1.7.1          | 17 KB     | ########## | 100% \n",
      "setuptools-46.0.0    | 635 KB    | ########## | 100% \n",
      "libstdcxx-ng-9.2.0   | 4.5 MB    | ########## | 100% \n",
      "boost-cpp-1.72.0     | 21.8 MB   | ########## | 100% \n",
      "typed-ast-1.4.1      | 205 KB    | ########## | 100% \n",
      "glib-2.58.3          | 3.3 MB    | ########## | 100% \n",
      "grpc-cpp-1.27.3      | 4.4 MB    | ########## | 100% \n",
      "ipykernel-5.1.4      | 160 KB    | ########## | 100% \n",
      "libevent-2.1.10      | 1.3 MB    | ########## | 100% \n",
      "glog-0.4.0           | 104 KB    | ########## | 100% \n",
      "cffi-1.14.0          | 221 KB    | ########## | 100% \n",
      "expat-2.2.9          | 191 KB    | ########## | 100% \n",
      "pyrsistent-0.15.7    | 89 KB     | ########## | 100% \n",
      "jupyter_core-4.6.3   | 71 KB     | ########## | 100% \n",
      "pydata-google-auth-0 | 13 KB     | ########## | 100% \n",
      "c-ares-1.15.0        | 100 KB    | ########## | 100% \n",
      "idna-2.9             | 52 KB     | ########## | 100% \n",
      "libffi-3.2.1         | 46 KB     | ########## | 100% \n",
      "pylint-2.4.4         | 424 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... b'Enabling notebook extension jupyter-js-widgets/extension...\\n      - Validating: \\x1b[32mOK\\x1b[0m\\n'\n",
      "done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate simonyi_workshop\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "Removing intermediate container 2b8d4641ffd4\n",
      " ---> e2d4bee20a4e\n",
      "Step 4/6 : FROM builder\n",
      " ---> e2d4bee20a4e\n",
      "Step 5/6 : ADD scoring.py shutdown.py trained_pipe.pkl /\n",
      " ---> 3a043bf9743b\n",
      "Step 6/6 : ENTRYPOINT [\"conda\", \"run\", \"-n\", \"simonyi_workshop\", \"python\", \"scoring.py\"]\n",
      " ---> Running in 84656b7e81d0\n",
      "Removing intermediate container 84656b7e81d0\n",
      " ---> 07d53fdb1311\n",
      "Successfully built 07d53fdb1311\n",
      "Successfully tagged gcr.io/norbert-liki-sandbox/home_credit_scoring:latest\n",
      "PUSH\n",
      "Pushing gcr.io/norbert-liki-sandbox/home_credit_scoring:latest\n",
      "The push refers to repository [gcr.io/norbert-liki-sandbox/home_credit_scoring]\n",
      "8c1961b4cfd0: Preparing\n",
      "fc6ad5bc0f8c: Preparing\n",
      "e1289794f1c9: Preparing\n",
      "fcd8d39597dd: Preparing\n",
      "875120aa853c: Preparing\n",
      "f2cb0ecef392: Preparing\n",
      "f2cb0ecef392: Waiting\n",
      "fcd8d39597dd: Layer already exists\n",
      "875120aa853c: Layer already exists\n",
      "f2cb0ecef392: Layer already exists\n",
      "8c1961b4cfd0: Pushed\n",
      "e1289794f1c9: Pushed\n",
      "fc6ad5bc0f8c: Pushed\n",
      "latest: digest: sha256:ef0946304ac434b2cc775aaad5194fdb61ce9bcbb9381166ed3fe4bc5c564d7f size: 1584\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                          IMAGES                                                     STATUS\n",
      "afbb3a86-cfdf-43ae-89da-723db5aac3a8  2020-03-13T18:26:59+00:00  8M30S     gs://norbert-liki-sandbox_cloudbuild/source/1584123969.16-ad85cd62f5ba46f395e71cb256fe3bfb.tgz  gcr.io/norbert-liki-sandbox/home_credit_scoring (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit . -t \"gcr.io/$PROJECT_ID/home_credit_scoring:latest\" --project=$PROJECT_ID --timeout=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying the image to a Compute Engine instance\n",
    "\n",
    "Using our freshly built container we can deploy it as a background running service simply with a simple gcloud command.\n",
    "Note that we are using the preemptbile flag. With this we can save money. These instances cost almost 80% less than normal ones. On the other hand they could be shutdown anytime and stay up maximum 24 hours.\n",
    "\n",
    "**You can use this logic to create and long running computation in the background.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             ZONE            MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP   STATUS\n",
      "credit-scoring3  europe-west4-a  n1-standard-1  true         10.164.0.18  34.91.119.55  RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created [https://www.googleapis.com/compute/v1/projects/norbert-liki-sandbox/zones/europe-west4-a/instances/credit-scoring3].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud compute instances create-with-container credit-scoring3 \\\n",
    "--container-image=\"gcr.io/norbert-liki-sandbox/home_credit_scoring:latest\" \\\n",
    "--project=norbert-liki-sandbox \\\n",
    "--zone=europe-west4-a \\\n",
    "--scopes=https://www.googleapis.com/auth/cloud-platform \\\n",
    "--maintenance-policy=TERMINATE \\\n",
    "--preemptible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 14000/14000 [00:01<00:00, 8868.70rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SK_ID_CURR  prediction                             time\n",
      "0      268490    0.000000 2020-03-13 16:13:00.288943+00:00\n",
      "1      401057    0.033122 2020-03-13 16:13:00.288943+00:00\n",
      "2      166801    0.128110 2020-03-13 16:13:00.288943+00:00\n",
      "3      130052    0.018069 2020-03-13 16:13:00.288943+00:00\n",
      "4      224534    0.444930 2020-03-13 16:13:00.288943+00:00\n",
      "--------------- Prediction records by date.\n",
      "time\n",
      "2020-03-12 13:39:44.305047+00:00    2000\n",
      "2020-03-12 13:41:29.782480+00:00    2000\n",
      "2020-03-12 13:45:28.364581+00:00    2000\n",
      "2020-03-12 13:48:38.421967+00:00    2000\n",
      "2020-03-13 16:10:00.333668+00:00    2000\n",
      "2020-03-13 16:13:00.288943+00:00    2000\n",
      "2020-03-13 18:38:57.188150+00:00    2000\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validate_bq_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
